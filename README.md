
# Amazon Sales Data Pipeline with Snowflake, Snowpark, and Airflow (Astro)

## ğŸ“ˆ Project Overview

This project demonstrates an **end-to-end data pipeline** using:

- **Airflow (Astro)** for orchestration
- **Snowflake & Snowpark** for scalable data processing
- **Local Amazon sales CSV data** as the source

The pipeline **extracts Amazon sales data from your local system, uploads it to a Snowflake stage, transforms it using Snowpark, and loads it into Snowflake tables for analytics-ready consumption.**

---

## ğŸ› ï¸ Tech Stack

- **Airflow (Astro CLI)**: Orchestration
- **Snowflake**: Cloud Data Warehouse
- **Snowpark**: In-Snowflake processing
- **Python**: ETL scripts
- **Docker**: Containerization (via Astro)
- **Git**: Version control

---

## ğŸ“‚ Project Structure

\`\`\`
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ amazon_sales_pipeline.py        # Airflow DAG for orchestrating the pipeline
â”œâ”€â”€ include/
â”‚   â””â”€â”€ sample_amazon_sales.csv         # Sample local sales data
â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ snowflake_helpers.py            # Helper functions for Snowflake connection and Snowpark processing
â”œâ”€â”€ requirements.txt                    # Python dependencies
â””â”€â”€ README.md                           # Project documentation
\`\`\`

---

## âš¡ Pipeline Flow

1ï¸âƒ£ **Extract:**  
Load Amazon sales data from local CSV into Snowflake stage using Snowflake Python Connector.

2ï¸âƒ£ **Transform:**  
Use **Snowpark** to clean, transform, and enrich the data inside Snowflake.

3ï¸âƒ£ **Load:**  
Load the transformed data into analytics-ready Snowflake tables.

4ï¸âƒ£ **Orchestrate:**  
The entire workflow is managed by **Airflow (Astro)** for easy scheduling, monitoring, and scaling.

---

## ğŸš€ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

\`\`\`bash
git clone https://github.com/yourusername/amazon-sales-pipeline.git
cd amazon-sales-pipeline
\`\`\`

---

### 2ï¸âƒ£ Install Astro CLI

Follow official instructions:  
[Astro CLI Installation](https://docs.astronomer.io/astro/cli/install-cli)

---

### 3ï¸âƒ£ Configure Snowflake Connection

Add your Snowflake connection to Airflow:

\`\`\`bash
astro dev init
\`\`\`

Update your \`airflow_settings.yaml\` or add the connection manually in the Airflow UI:

- **Conn ID:** \`snowflake_conn\`
- **Conn Type:** \`Snowflake\`
- **Account:** \`your_account\`
- **User:** \`your_user\`
- **Password:** \`your_password\`
- **Database:** \`your_database\`
- **Warehouse:** \`your_warehouse\`
- **Schema:** \`your_schema\`

---

### 4ï¸âƒ£ Start the Astro Dev Environment

\`\`\`bash
astro dev start
\`\`\`

Access Airflow at [http://localhost:8080](http://localhost:8080).

---

### 5ï¸âƒ£ Trigger the DAG

- Go to the Airflow UI.
- Enable and trigger the \`amazon_sales_pipeline\` DAG.
- Monitor task execution and logs.

---

## ğŸ§ª Testing

- **Unit tests** for Snowflake/Snowpark logic can be added using \`pytest\`.
- Validate the loaded data in Snowflake using SQL Workbench or Snowflake UI.

---

## ğŸ“Š Future Enhancements

âœ… Add data quality checks using **Great Expectations**  
âœ… Parameterize file paths and table names for flexibility  
âœ… Integrate **dbt for transformation layer**  
âœ… Add **notifications on pipeline failures**  

---

## ğŸ¤ Contributing

PRs are welcome! Please fork the repo and submit pull requests for any improvements.

---

## ğŸ“ License

This project is licensed under the MIT License.

---

## ğŸ“¬ Contact

For queries or collaborations, reach out via [vatsanikhil@gmail.com](mailto:vatsanikhil@gmail.com) or [LinkedIn](https://www.linkedin.com/in/nikhil-vatsa-29960517b/).

---

### â­ If you find this helpful, please consider giving the repository a star!
