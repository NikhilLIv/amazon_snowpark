# Amazon Sales Data Pipeline with Snowflake, Snowpark, and Astro (Airflow)

## üìà Project Overview

This repository contains an **end-to-end data engineering pipeline** using:

- **Snowflake** (data warehousing, staging, transformation)
- **Snowpark** (Python-based scalable transformations inside Snowflake)
- **Astro (Astronomer Airflow)** for orchestration and scheduling

The pipeline ingests **Amazon Sales Data** from **local CSV files** for:
- India (`amazon_sales_india.csv`)
- France (`amazon_sales_france.csv`)
- US (`amazon_sales_us.csv`)

and transforms them into a **curated star schema** in Snowflake for analytics and reporting.

---

## üóÇÔ∏è Pipeline Stages

1. **Ingestion**: Upload local Amazon sales CSV files to a **Snowflake stage**.
2. **Loading to Source Table**: Load data from the stage into **raw source tables**.
3. **Curated Transformations**: Clean, enrich, and unify data using **Snowpark Python scripts**.
4. **Star Schema Creation**: Load curated data into **fact and dimension tables**:
   - `fact_sales`
   - `dim_customer`
   - `dim_product`
   - `dim_date`
5. **Orchestration with Astro**:
   - Automated scheduling and execution using **Astro Airflow**
   - Logging and monitoring
   - Modular task separation for ingestion, transformation, and loading

---

## üõ†Ô∏è Tech Stack

- **Snowflake**: Data warehouse for staging, transformation, and storage
- **Snowpark (Python)**: In-database scalable transformations
- **Astro (Astronomer Airflow)**: Orchestration and scheduling
- **Python**: For Snowpark scripts and Airflow DAGs

---

## üöÄ Setup Instructions

### 1Ô∏è‚É£ Prerequisites

- Snowflake account with appropriate role (`SYSADMIN`, `ACCOUNTADMIN`, or pipeline role)
- **Astro CLI** installed ([Install guide](https://docs.astronomer.io/astro/cli/install-cli))
- An Astronomer workspace with Airflow deployment
- Python 3.8+ locally for development
- Snowflake credentials ready

---

### 2Ô∏è‚É£ Snowflake Configuration

- Create a **stage** in Snowflake for file ingestion.
- Create databases and schemas:
  - `SOURCE`
  - `CURATED`
  - `CONSUMPTION`
- Add your Snowflake connection in Astro:
  - Via the **Astro UI** under `Admin -> Connections`, or
  - In your local `.env` for local development.

---

### 3Ô∏è‚É£ File Structure

